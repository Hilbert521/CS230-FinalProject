{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1347,
     "output_extras": [
      {
       "item_id": 15
      },
      {
       "item_id": 43
      },
      {
       "item_id": 72
      },
      {
       "item_id": 101
      },
      {
       "item_id": 126
      },
      {
       "item_id": 167
      },
      {
       "item_id": 208
      },
      {
       "item_id": 246
      },
      {
       "item_id": 276
      },
      {
       "item_id": 301
      },
      {
       "item_id": 316
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 259308,
     "status": "ok",
     "timestamp": 1519418932673,
     "user": {
      "displayName": "David Troner",
      "photoUrl": "//lh6.googleusercontent.com/-Qsk0YFyokns/AAAAAAAAAAI/AAAAAAAAAA8/SH9fYhiAFZ0/s50-c-k-no/photo.jpg",
      "userId": "113969831701106809941"
     },
     "user_tz": 480
    },
    "id": "kBUqTo6HWZO3",
    "outputId": "2bd196c1-f4c7-48a8-eb67-87d1fd4095dc"
   },
   "outputs": [],
   "source": [
    "def DataLoad(ID,name):\n",
    "  # Code to read csv file into colaboratory:\n",
    "  !pip install -U -q PyDrive\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "\n",
    "  # 1. Authenticate and create the PyDrive client.\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "\n",
    "  #2. Get the file\n",
    "  downloaded = drive.CreateFile({'id':ID}) # replace the id with id of file you want to access\n",
    "  downloaded.GetContentFile(name)  \n",
    "\n",
    "  #3. Read file as panda dataframe\n",
    "  import pandas as pd\n",
    "  DATA = pd.read_csv(name)\n",
    "  DATA = DATA.values\n",
    "  return DATA\n",
    "\n",
    "def LoadDATA():\n",
    "  import numpy as np\n",
    "\n",
    "  ## Load Data\n",
    "  Obama1_ID_X = '1hNPmTEK1LNv02SPRzvLFegAb_suI7gBy'       #'1j8xblrjXOuAy2S_v4Qlo_hPJ4BWSUME5'\n",
    "  Obama1_name_X = 'obama_vid1_test_X'                     #'obama1_X.csv'\n",
    "  Obama1_ID_Y = '1sHhz5iF065NrB6MntDzFaXxSwCxY8Y-o'       #''1e8LfCcNRDJkzOaQKBO9HHNqU4DV_DPCh'\n",
    "  Obama1_name_Y = 'obama_vid1_test_Y.csv'\n",
    "  Trump1_ID_X =  '1ngP4wv6Ev1J_P_xYqPj4ejHvC-8ZQv3c'      # '1Xtp9-M76UjcHnqTYpeawUM6zLHznrp-B'\n",
    "  Trump1_name_X = 'trump_vid1_test_X'                     #'trump1_X.csv'\n",
    "  Trump1_ID_Y =    '1s-jnyN9ab_RyMiN80t6Zmk9zkViAF_AH'    #'1mVVuBg3HKsJ2qX-nu_sq8bAjNZhsxUfm'\n",
    "  Trump1_name_Y = 'trump_vid1_test_Y'                     #'trump1_Y.csv'\n",
    "  trainPercent = 0.95 # % of samples to train on\n",
    "\n",
    "  Obama1_X_DATASET = DataLoad(Obama1_ID_X,Obama1_name_X)\n",
    "  Obama1_Y_DATASET = DataLoad(Obama1_ID_Y,Obama1_name_Y)\n",
    "  Trump1_X_DATASET = DataLoad(Trump1_ID_X,Trump1_name_X)\n",
    "  Trump1_Y_DATASET = DataLoad(Trump1_ID_Y,Trump1_name_Y)\n",
    "  X_DATASET = np.vstack((Obama1_X_DATASET,Trump1_X_DATASET))\n",
    "  Y_DATASET = np.vstack((Obama1_Y_DATASET,Trump1_Y_DATASET))\n",
    "  DATASET = np.hstack((X_DATASET,Y_DATASET))\n",
    "  np.random.shuffle(DATASET)\n",
    "  X_DATASET = DATASET[:,0:X_DATASET.shape[1]]\n",
    "  Y_DATASET = DATASET[:,X_DATASET.shape[1]:DATASET.shape[1]]\n",
    "\n",
    "  mtrain = int(trainPercent*X_DATASET.shape[0])\n",
    "  x_train = X_DATASET[0:mtrain,0:X_DATASET.shape[1]]\n",
    "  x_test  = X_DATASET[mtrain:X_DATASET.shape[0],0:X_DATASET.shape[1]]\n",
    "  y_train = Y_DATASET[0:mtrain,0:Y_DATASET.shape[1]]\n",
    "  y_test  = Y_DATASET[mtrain:Y_DATASET.shape[0],0:Y_DATASET.shape[1]]\n",
    "  return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def NNcode(x):\n",
    "  !pip install -q keras\n",
    "  import keras\n",
    "  from keras.models import Sequential\n",
    "  from keras.layers import Dense, Activation, BatchNormalization\n",
    "  from keras.optimizers import SGD\n",
    "  from keras import regularizers, optimizers\n",
    "  import numpy as np\n",
    "  import tensorflow as tf\n",
    "  \n",
    "  Lparam     = int(x[0])\n",
    "  bsizeparam = int(x[1])\n",
    "  L2param    = int(x[2])/100\n",
    "  Nparam     = int(x[3])\n",
    "  \n",
    "  #################### Hyperparameters ####################\n",
    "  show_plots = 0                   # Bool, to show training plots\n",
    "  L = Lparam                       # num layers in network\n",
    "  bsize = bsizeparam               # minibatch size\n",
    "  epochnum = 10                    # num epochs for optimization\n",
    "  learning_rate = 0.001            # params for Adam\n",
    "  learning_decay = 0.00001         # learning decay for Adam\n",
    "  BN_mom = 0.99                    # momentum for batch norm\n",
    "  L2 = np.ones(L)*L2param          # L2 lambda parameters for each layer\n",
    "  N = np.append(np.ones(L-1)*Nparam,classes)   # num nodes/layer for each hidden layer\n",
    "  #########################################################\n",
    "  params = {}\n",
    "  \n",
    "  for l in range(1,L+1):\n",
    "      params[\"L2_\"+str(l)] = int(L2[l-1]*100)/100\n",
    "      params[\"N\"+str(l)] = int(N[l-1])\n",
    "\n",
    "  ## NN Model (X->RELU->RELU->RELU->RELU->SOFTMAX)\n",
    "  model = Sequential()\n",
    "  model.add(Dense(params['N1'], activation='relu', input_dim=n_vec,use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(params[\"L2_1\"]), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None,name='fc1'))\n",
    "  model.add(BatchNormalization(axis=1, momentum=BN_mom, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones',name='bn1'))\n",
    "  for l in range(2,L-1):\n",
    "      model.add(Dense(params['N'+str(l)], activation='relu',use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(params[\"L2_\" + str(l)]), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None,name='fc'+str(l)))\n",
    "      model.add(BatchNormalization(axis=1, momentum=BN_mom, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones',name='bn'+str(l)))\n",
    "  model.add(Dense(params['N'+str(L)], activation='softmax',use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(params[\"L2_\" + str(L)]), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None,name='fc'+str(L)))\n",
    "  \n",
    "  \n",
    "  ## Compile and Train Model\n",
    "  adam_opt = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=learning_decay, amsgrad=False)\n",
    "  model.compile(loss='categorical_crossentropy',optimizer=adam_opt,metrics=['accuracy'])\n",
    "  history = model.fit(x_train, y_train,batch_size=bsize,epochs=epochnum,verbose=0)\n",
    "  \n",
    "  ## Plots\n",
    "  if show_plots == 1:\n",
    "    import matplotlib.pyplot as plt\n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "  \n",
    "  ## Test Model\n",
    "  preds = model.evaluate(x_test, y_test, batch_size=bsize)\n",
    "  print (\"Loss = \" + str(preds[0]))\n",
    "  print (\"Test Accuracy = \" + str(preds[1]))\n",
    "  print(\"with X = \" + str(x.T))\n",
    "  \n",
    "\n",
    "  ## Model Predictions of Test Set\n",
    "  y_hat = model.predict(x_test, batch_size=bsize)\n",
    "  return preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vd74nlg5aCWW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grid Search over Hyperparameters\n",
    "import numpy as np\n",
    "x_train,x_test,y_train,y_test = LoadDATA()\n",
    "classes = y_train.shape[1]  # num speakers\n",
    "n_vec =   x_train.shape[1]  # length of feature vector\n",
    "m_train = x_train.shape[0]  # num training examples\n",
    "m_test =  x_test.shape[0]   # num test examples\n",
    "\n",
    "L_list = [5,10]#[5,7,10,15,20]\n",
    "Bsize_list = [16,64]#[16,32,64,128,256]\n",
    "L2_list = [.1,.5]#[.1,.2,.3,.4,.5]\n",
    "N_list = [10,390]#[10,20,50,100,390,500,1000]\n",
    "\n",
    "len1 = len(L_list)\n",
    "len2 = len(Bsize_list)\n",
    "len3 = len(L2_list)\n",
    "len4 = len(N_list)\n",
    "len_total = len1*len2*len3*len4\n",
    "count = 0\n",
    "x = np.zeros((4,1))\n",
    "params = np.zeros((len_total,4))\n",
    "test_acc = np.zeros((len_total,1))\n",
    "\n",
    "for i1 in range(len1):\n",
    "  for i2 in range(len2):\n",
    "    for i3 in range(len3):\n",
    "      for i4 in range(len4):\n",
    "        x[0] = L_list[i1]\n",
    "        x[1] = Bsize_list[i2]\n",
    "        x[2] = L2_list[i3]\n",
    "        x[3] = N_list[i4]\n",
    "        test_acc[count] = NNcode(x)\n",
    "        params[count,:] = np.array([x[0],x[1],x[2],x[3]]).T        \n",
    "        print('Run '+str(count+1)+' out of '+str(len_total))\n",
    "        count = count+1\n",
    "max_ind = np.argmax(test_acc)\n",
    "print('Max Accuracy = '+str(test_acc[max_ind,0]*100)+'%')\n",
    "print('with L = '+str(params[max_ind,0])+', Bsize = '+str(params[max_ind,1])+', L2 = '+str(params[max_ind,2])+', N = '+str(params[max_ind,3]))\n",
    "\n",
    "## Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X1 = params[:,0]\n",
    "X2 = params[:,1]\n",
    "X3 = params[:,2]\n",
    "X4 = params[:,3]\n",
    "Y = test_acc\n",
    "\n",
    "plt.plot(X1,Y,marker='o',linestyle='None',markersize=12)\n",
    "plt.title('Test Accuracy vs NN Length')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('NN Length')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X2,Y,marker='o',linestyle='None',markersize=12)\n",
    "plt.title('Test Accuracy vs Batch Size')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X3,Y,marker='o',linestyle='None',markersize=12)\n",
    "plt.title('Test Accuracy vs L2 Lambda')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Lambda')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X4,Y,marker='o',linestyle='None',markersize=12)\n",
    "plt.title('Test Accuracy vs Nodes/Layer')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Nodes/Layer')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "ProjectCodeIDK.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
